{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy dataset for a sparse reward RL environment, and traing a preference learning model for it\n",
    "# Created by Alexander Li, 2023-12-07\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The environment is a 1D line from -2.0 to 2.0, with a goal at location 1.0, and a grid size of 0.01\n",
    "# The agent starts at a random location, with the option to move left or right by 0.01, and a slight bias to move towards the goal\n",
    "# The reward is 1.0 if the agent reaches the goal, and 0.0 otherwise \n",
    "# The agent is terminated after 3000 steps\n",
    "\n",
    "# create a dataset of 1000 trajectories of 3000 steps each as a 1D vector\n",
    "goal_location = 1.0\n",
    "goal_threshold = 0.05\n",
    "world_size = 2.0\n",
    "grid_size = 0.01\n",
    "step_size = 0.01\n",
    "direction_bias = 0.1\n",
    "n_eps = 1000\n",
    "eps_length = 3000\n",
    "dataset = []\n",
    "# for every episode\n",
    "for i in range(n_eps):\n",
    "    # add a new trajectory\n",
    "    trajectory = []\n",
    "    # start at a random location\n",
    "    start_location = np.random.uniform(-world_size, world_size)\n",
    "    # round to the nearest grid point\n",
    "    start_location = np.round(start_location / grid_size) * grid_size\n",
    "    # append a tuple of the time, start location, and reward)\n",
    "    trajectory.append((0, start_location, 0.0))\n",
    "\n",
    "    # for every time step\n",
    "    for j in range(1, eps_length):\n",
    "        # old location\n",
    "        old_location = trajectory[j-1][1]\n",
    "        \n",
    "        # add a slight bias to move towards the goal\n",
    "        if np.random.uniform() < direction_bias:\n",
    "            # if the agent is to the left of the goal, move right\n",
    "            if old_location < goal_location:\n",
    "                new_location = old_location + step_size\n",
    "            # if the agent is to the right of the goal, move left\n",
    "            else:\n",
    "                new_location = old_location - step_size\n",
    "        else:\n",
    "            # move left or right by the step size\n",
    "            new_location = old_location + np.random.choice([-step_size, step_size])\n",
    "\n",
    "        # terminate if the agent reaches the goal within a threshold\n",
    "        if np.abs(new_location - goal_location) < goal_threshold:\n",
    "            trajectory.append((j, new_location, 1.0))\n",
    "            # append trajectory to dataset\n",
    "            dataset.append(trajectory)\n",
    "            break\n",
    "        else:\n",
    "            # append tuple of time, location, and reward\n",
    "            trajectory.append((j, new_location, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of the dataset is: \" + str(len(dataset)))\n",
    "print(\"The length of the first trajectory is: \" + str(len(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 5 sample trajectories on the same plot, including the goal line\n",
    "plt.axhline(y=goal_location, color='r', linestyle='-')\n",
    "# include the error region\n",
    "plt.axhspan(goal_location - goal_threshold, goal_location + goal_threshold, alpha=0.5, color='r')\n",
    "# plot the 5 trajectories\n",
    "for i in range(5):\n",
    "    plt.plot([x[0] for x in dataset[i]], [x[1] for x in dataset[i]], label=f\"trajectory {i+1}\")\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('Sample Trajectories')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# create a preference dataset from the trajectories\n",
    "# one tensor for sampling 2 states, one tensor for the preference\n",
    "# the preference is 1 if the time of the first state is closer to the goal than the time of the second state\n",
    "# the preference is 0 if the time of the second state is closer to the goal than the time of the first state\n",
    "# number of samples \n",
    "n_samples = 100000\n",
    "state_pairs = torch.zeros((n_samples, 2))\n",
    "preferences = torch.zeros((n_samples, 1))\n",
    "for i in range(n_samples):\n",
    "    # randomly select an episode\n",
    "    episode = np.random.choice(dataset)\n",
    "    # randomly generate 2 indices from that episode\n",
    "    idx1, idx2 = np.random.choice(len(episode), 2)\n",
    "    # randomly select two states from that episode\n",
    "    state1, state2 = episode[idx1], episode[idx2]\n",
    "    # extract the time and location of each state\n",
    "    t1, t2 = state1[0], state2[0]\n",
    "    x1, x2 = state1[1], state2[1]\n",
    "    t_goal = episode[-1][0]\n",
    "    # assign the states to the tensor\n",
    "    state_pairs[i, 0] = x1\n",
    "    state_pairs[i, 1] = x2\n",
    "    # if the first state is closer to the goal\n",
    "    if (t_goal - t1) < (t_goal - t2):\n",
    "        preferences[i, 0] = 1.0\n",
    "    # if the second state is closer to the goal\n",
    "    else:\n",
    "        preferences[i, 0] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the state pairs are: \\n\" + str(state_pairs.size()))\n",
    "print(\"The state pairs are: \" + str(state_pairs))\n",
    "print(\"\")\n",
    "print(\"The shape of the preferences are: \\n\" + str(preferences.size()))\n",
    "print(\"The preferences are: \" + str(preferences))\n",
    "\n",
    "# visualise the dataset\n",
    "plt.scatter(state_pairs[:, 0], state_pairs[:, 1], c=preferences[:, 0], cmap=\"bwr\")\n",
    "plt.xlabel('state 1')\n",
    "plt.ylabel('state 2')\n",
    "plt.title('Preference Labels of the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the dataset into a dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, state_pairs, preferences):\n",
    "        self.state_pairs = state_pairs\n",
    "        self.preferences = preferences\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.state_pairs[index], self.preferences[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.state_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PreferenceDataset(state_pairs, preferences)\n",
    "# split the dataset into train, validation and test\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "# create dataloaders\n",
    "batch_size = 200\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# print the shapes of batches from dataloaders\n",
    "batch = next(iter(train_loader))\n",
    "print(\"The shape of the input batch: \" + str(batch[0].shape))\n",
    "print(\"The shape of the batch labels: \" + str(batch[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# create a preference learning model\n",
    "class PreferenceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PreferenceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # apply the linear layers to each state in the state pair\n",
    "        x1 = self.sigmoid(self.fc1(x[:, 0].unsqueeze(1)))\n",
    "        x1 = self.sigmoid(self.fc2(x1))\n",
    "        x1 = self.fc3(x1)\n",
    "        # split the output as 2 values, one as the predicted reward, the other as the uncertainty in the predicted reward\n",
    "        u1 = x1[:, 0].unsqueeze(1)\n",
    "        #s1 = torch.abs(x1[:, 1].unsqueeze(1)) # make the standard deviation always positive\n",
    "        # repeat for state 2\n",
    "        x2 = self.sigmoid(self.fc1(x[:, 1].unsqueeze(1)))\n",
    "        x2 = self.sigmoid(self.fc2(x2))\n",
    "        x2 = self.fc3(x2)\n",
    "        # split the output as 2 values, one as the predicted reward, the other as the uncertainty in the predicted reward\n",
    "        u2 = x2[:, 0].unsqueeze(1)\n",
    "        #s2 = torch.abs(x2[:, 1].unsqueeze(1)) # make the standard deviation always positive\n",
    "        # sample a reward from a normal distribution with the predicted reward and uncertainty\n",
    "        #r1 = torch.normal(u1, s1)\n",
    "        #r2 = torch.normal(u2, s2)\n",
    "        # return the sampled preference\n",
    "        return torch.sigmoid(u1 - u2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "\n",
    "# training step\n",
    "def training_step(input_batch, target_batch, model, optimizer, loss_func):\n",
    "    preds = model(input_batch) # forward pass\n",
    "    loss = loss_fn(preds, target_batch) # find loss\n",
    "    optimizer.zero_grad() # clear gradients in the optimiser\n",
    "    loss.backward() # back prop\n",
    "    optimizer.step() # gradient descend\n",
    "    return loss.item() # return loss\n",
    "\n",
    "# validation step\n",
    "def validation_step(input_batch, target_batch, model, loss_func):\n",
    "    preds = model(input_batch) # forward pass\n",
    "    loss = loss_func(preds, target_batch) # find loss\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "model = PreferenceModel()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "def loss_fn(preds, target):\n",
    "    return F.binary_cross_entropy(preds, target)\n",
    "\n",
    "num_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to get the next available filename\n",
    "def get_next_filename(base_name, extension):\n",
    "    index = 1\n",
    "    while True:\n",
    "        filename = f\"{base_name}_{index}.{extension}\"\n",
    "        if not os.path.exists(filename):\n",
    "            return filename\n",
    "        index += 1\n",
    "\n",
    "base_filename = 'best_model'\n",
    "file_extension = 'pth'\n",
    "next_filename = get_next_filename(base_filename, file_extension)\n",
    "\n",
    "\n",
    "# train\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        train_loss += training_step(input_batch, target_batch, model, optimizer, loss_fn)\n",
    "    # validation\n",
    "    model.eval()\n",
    "    for input_batch, target_batch in val_loader:\n",
    "        val_loss += validation_step(input_batch, target_batch, model, loss_fn)\n",
    "        \n",
    "    # normalise the losses\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    # Check if validation loss has decreased\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        # Save the model state\n",
    "        torch.save(model.state_dict(), next_filename)\n",
    "    \n",
    "    # append the losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    # print the losses\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='val loss')  \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Losses')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the reward function\n",
    "# create a batch of tensor of states from -2.0 to 2.0 with a step size of 0.01\n",
    "# only use the linear layers of the model to find the reward tensor\n",
    "# then plot the reward tensor\n",
    "states = torch.arange(-2.0, 2.0, 0.01).unsqueeze(1)\n",
    "rewards = model.sigmoid(model.fc1(states))\n",
    "rewards = model.sigmoid(model.fc2(rewards))\n",
    "rewards = model.fc3(rewards)\n",
    "\n",
    "states = states[:, 0].detach().numpy()\n",
    "rewards = rewards[:, 0].detach().numpy()\n",
    "plt.plot(states, rewards)\n",
    "plt.xlabel('state')\n",
    "plt.ylabel('reward')\n",
    "plt.title('Distributed Reward Function')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
