{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_env import Maze\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "def get_latest_folder(base_name):\n",
    "    index = 1\n",
    "    while True:\n",
    "        folder = f\"{base_name}_{index}\"\n",
    "        if not os.path.exists(folder):\n",
    "            folder = f\"{base_name}_{index-1}\"\n",
    "            return folder\n",
    "        index += 1\n",
    "folder = get_latest_folder(\"world\")\n",
    "\n",
    "from maze_PTR_model import MazePTRModel\n",
    "model = MazePTRModel()\n",
    "checkpoint = torch.load(f\"{folder}/models/last_model.pt\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "# iterations\n",
    "iters = 50000\n",
    "initial_epsilon = 1.0\n",
    "eps_anneal_rate = 0.3\n",
    "alpha= 0.5\n",
    "gamma = 0.99\n",
    "eval_episodes = 1000\n",
    "eval_steps = 100\n",
    "\n",
    "move_penalty = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new env with estimated reward and train agent\n",
    "shrink_factor = 10\n",
    "env_1 = Maze(sparse=False, model=model, move_penalty=move_penalty)\n",
    "#Q_2 = env_2.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2 = Maze(sparse=False, model=model, move_penalty=move_penalty)\n",
    "env_2.shrink_reward(shrink_factor/4)\n",
    "env_2.plot_reward()\n",
    "#Q_3 = env_3.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_3 = Maze(sparse=False, model=model, move_penalty=move_penalty)\n",
    "env_3.shrink_reward(shrink_factor/2)\n",
    "env_3.plot_reward()\n",
    "#Q_4 = env_4.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_4 = Maze(sparse=False, model=model, move_penalty=move_penalty)\n",
    "env_4.shrink_reward(shrink_factor)\n",
    "env_4.plot_reward()\n",
    "#Q_5 = env_5.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new env with estimated reward and train agent\n",
    "env_5 = Maze(sparse=True, move_penalty=move_penalty)\n",
    "env_5.plot_reward()\n",
    "#Q_1 = env_1.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train agent on all envs, evaluate performance every 10 episodes, produce 5 arrays of performances and plot\n",
    "num_episodes = 3000\n",
    "eval_frequency = 20\n",
    "episode_length = 1000\n",
    "e = 0.3\n",
    "s = 200\n",
    "eps = np.arange(0, num_episodes, eval_frequency)\n",
    "performance_1 = np.zeros(len(eps))\n",
    "performance_2 = np.zeros(len(eps))\n",
    "performance_3 = np.zeros(len(eps))\n",
    "performance_4 = np.zeros(len(eps))\n",
    "performance_5 = np.zeros(len(eps)) \n",
    "Q_1 = np.ones((env_1.Nx*env_1.Ny,env_1.action_space.shape[0]))\n",
    "Q_2 = np.ones((env_2.Nx*env_2.Ny,env_2.action_space.shape[0]))\n",
    "Q_3 = np.ones((env_3.Nx*env_3.Ny,env_3.action_space.shape[0]))\n",
    "Q_4 = np.ones((env_4.Nx*env_4.Ny,env_4.action_space.shape[0]))\n",
    "Q_5 = np.ones((env_5.Nx*env_5.Ny,env_5.action_space.shape[0]))\n",
    "for i in range(len(eps)):\n",
    "    print(f\"batch {i}, learning Q\")\n",
    "    Q_1 = env_1.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_1, episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "    Q_2 = env_2.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_2, episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "    Q_3 = env_3.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_3, episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "    Q_4 = env_4.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_4, episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "    Q_5 = env_5.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_5, episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "    print(f\"batch {i}, evaluating performance\") \n",
    "    performance_1[i] = env_1.evaluate_Q(Q=Q_1, episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=True)\n",
    "    performance_2[i] = env_2.evaluate_Q(Q=Q_2, episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=True)\n",
    "    performance_3[i] = env_3.evaluate_Q(Q=Q_3, episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=True)\n",
    "    performance_4[i] = env_4.evaluate_Q(Q=Q_4, episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=True)\n",
    "    performance_5[i] = env_5.evaluate_Q(Q=Q_5, episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure()\n",
    "plt.plot(eps, performance_1, label=\"Dense\")\n",
    "plt.plot(eps, performance_2, label=\"Dense/2.5\")\n",
    "plt.plot(eps, performance_3, label=\"Dense/5\")\n",
    "plt.plot(eps, performance_4, label=\"Dense/10\")\n",
    "plt.plot(eps, performance_5, label=\"Sparse\")\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"performance (probability of reaching goal)\")\n",
    "plt.title(\"performance over time\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{folder}/figs/eval_over_time.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create side by side subplots of the 5 reward functions\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"Reward landscapes\", fontsize=16)\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(env_1.reward_landscape.T, origin='lower', cmap='gray')\n",
    "# hide the axes\n",
    "plt.axis('off')\n",
    "plt.title(\"Dense\")\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(env_2.reward_landscape.T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Dense/2.5\")\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(env_3.reward_landscape.T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Dense/5\")\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(env_4.reward_landscape.T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Dense/10\")\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(env_5.reward_landscape.T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Sparse\")\n",
    "plt.savefig(f\"{folder}/figs/rewards_compare.png\")\n",
    "plt.show()\n",
    "\n",
    "# create side by side subplots of the 5 Q functions\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"Q landscapes\", fontsize=16)\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(np.argmax(Q_1,axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "# hide the axes\n",
    "plt.axis('off')\n",
    "plt.title(\"Dense\")\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(np.argmax(Q_2,axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Dense/2.5\")\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(np.argmax(Q_3,axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Dense/5\")\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(np.argmax(Q_4,axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Dense/10\")\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(np.argmax(Q_5,axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Sparse\")\n",
    "plt.savefig(f\"{folder}/figs/Q_compare.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
