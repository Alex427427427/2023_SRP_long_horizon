{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_env import Maze\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "def get_latest_folder(base_name):\n",
    "    index = 1\n",
    "    while True:\n",
    "        folder = f\"{base_name}_{index}\"\n",
    "        if not os.path.exists(folder):\n",
    "            folder = f\"{base_name}_{index-1}\"\n",
    "            return folder\n",
    "        index += 1\n",
    "#folder = get_latest_folder(\"world\")\n",
    "folder = \"world_1\"\n",
    "\n",
    "from maze_PTR_model import MazePTRModel\n",
    "model = MazePTRModel()\n",
    "checkpoint = torch.load(f\"{folder}/models/last_model.pt\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "# iterations\n",
    "iters = 50000\n",
    "initial_epsilon = 1.0\n",
    "eps_anneal_rate = 0.3\n",
    "alpha= 0.5\n",
    "gamma = 0.99\n",
    "eval_episodes = 1000\n",
    "eval_steps = 100\n",
    "\n",
    "move_penalty = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new env with estimated reward and train agent\n",
    "shrink_factor = 50\n",
    "env_1 = Maze(sparse=False, model=model, move_penalty=move_penalty)\n",
    "env_1.plot_reward()\n",
    "#Q_2 = env_2.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2 = Maze(sparse=False, model=model, move_penalty=move_penalty)\n",
    "env_2.shrink_reward(shrink_scaling=5)\n",
    "env_2.plot_reward()\n",
    "#Q_3 = env_3.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_3 = Maze(sparse=False, model=model, move_penalty=move_penalty)\n",
    "env_3.shrink_reward(shrink_scaling=20)\n",
    "env_3.plot_reward()\n",
    "#Q_4 = env_4.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_4 = Maze(sparse=False, model=model, move_penalty=move_penalty)\n",
    "env_4.shrink_reward(shrink_scaling=120)\n",
    "env_4.plot_reward()\n",
    "#Q_5 = env_5.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new env with estimated reward and train agent\n",
    "env_5 = Maze(sparse=True, move_penalty=move_penalty, goal_reward=100)\n",
    "env_5.plot_reward()\n",
    "#Q_1 = env_1.value_iter2(iters=iters, alpha=alpha,gamma=gamma,initial_eps=initial_epsilon, eps_anneal_rate=eps_anneal_rate,disp=True,plot_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train agent on all envs, evaluate performance every 10 episodes, produce 5 arrays of performances and plot\n",
    "num_episodes = 5000\n",
    "eval_frequency = 50\n",
    "episode_length = 500\n",
    "e = 0.2\n",
    "s = 400\n",
    "eps = np.arange(0, num_episodes, eval_frequency)\n",
    "n_Qs = 10\n",
    "perf_1 = np.zeros((n_Qs, len(eps)))\n",
    "perf_2 = np.zeros((n_Qs, len(eps)))\n",
    "perf_3 = np.zeros((n_Qs, len(eps)))\n",
    "perf_4 = np.zeros((n_Qs, len(eps)))\n",
    "perf_5 = np.zeros((n_Qs, len(eps)))\n",
    "perf_6 = np.zeros((n_Qs, len(eps))) # curriculum\n",
    "Q_1 = torch.ones(n_Qs, env_1.Nx*env_1.Ny, env_1.action_space.shape[0])\n",
    "Q_2 = torch.ones(n_Qs, env_2.Nx*env_2.Ny, env_2.action_space.shape[0])\n",
    "Q_3 = torch.ones(n_Qs, env_3.Nx*env_3.Ny, env_3.action_space.shape[0])\n",
    "Q_4 = torch.ones(n_Qs, env_4.Nx*env_4.Ny, env_4.action_space.shape[0])\n",
    "Q_5 = torch.ones(n_Qs, env_5.Nx*env_5.Ny, env_5.action_space.shape[0])\n",
    "Q_6 = torch.ones(n_Qs, env_1.Nx*env_1.Ny, env_1.action_space.shape[0]) # curriculum\n",
    "curriculum_advance_episode = 1000\n",
    "# print the progress\n",
    "total_passes = n_Qs*len(eps)\n",
    "for i in range(n_Qs):\n",
    "    print(f\"Repetition {i}\")\n",
    "    for epoch in range(len(eps)):\n",
    "        print(f\"epoch {epoch}, learning Q\")\n",
    "        Q_1[i] = env_1.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_1[i], episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "        Q_2[i] = env_2.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_2[i], episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "        Q_3[i] = env_3.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_3[i], episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "        Q_4[i] = env_4.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_4[i], episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "        Q_5[i] = env_5.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_5[i], episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "        if epoch < curriculum_advance_episode:\n",
    "            Q_6[i] = env_1.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_6[i], episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "        else:\n",
    "            Q_6[i] = env_5.train_Q_1_epoch(episodes=eval_frequency, init_Q=Q_6[i], episode_length=episode_length, alpha=alpha,gamma=gamma,final_greediness=0.5, eps_anneal=True,disp=False)\n",
    "        print(f\"epoch {epoch}, evaluating performance\") \n",
    "        perf_1[i, epoch] = env_1.evaluate_Q(Q=Q_1[i], episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=False)\n",
    "        perf_2[i, epoch] = env_2.evaluate_Q(Q=Q_2[i], episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=False)\n",
    "        perf_3[i, epoch] = env_3.evaluate_Q(Q=Q_3[i], episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=False)\n",
    "        perf_4[i, epoch] = env_4.evaluate_Q(Q=Q_4[i], episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=False)\n",
    "        perf_5[i, epoch] = env_5.evaluate_Q(Q=Q_5[i], episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=False)\n",
    "        perf_6[i, epoch] = env_1.evaluate_Q(Q=Q_6[i], episodes=20, episode_length=s, final_greediness=1-e, eps_anneal=False, use_start_zone=False)\n",
    "        passes = i*len(eps) + epoch + 1\n",
    "        print(f\"Progress: {passes}/{total_passes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the 5 performance arrays into a single tensor\n",
    "perf_1_ts = torch.tensor(perf_1)\n",
    "perf_2_ts = torch.tensor(perf_2)\n",
    "perf_3_ts = torch.tensor(perf_3)\n",
    "perf_4_ts = torch.tensor(perf_4)\n",
    "perf_5_ts = torch.tensor(perf_5)\n",
    "perf_6_ts = torch.tensor(perf_6)\n",
    "perf_tensor = torch.stack((perf_1_ts, perf_2_ts, perf_3_ts, perf_4_ts, perf_5_ts, perf_6_ts), dim=0)\n",
    "# find the next available folder\n",
    "def get_next_folder(base_name):\n",
    "    index = 1\n",
    "    while True:\n",
    "        folder = f\"{base_name}_{index}\"\n",
    "        if not os.path.exists(folder):\n",
    "            return folder\n",
    "        index += 1\n",
    "\n",
    "# create a folder called world\n",
    "eval_folder = get_next_folder(f\"{folder}/evaluation\")\n",
    "os.mkdir(eval_folder)\n",
    "# save the tensor to a file\n",
    "torch.save(perf_tensor, f\"{eval_folder}/performances.pt\")\n",
    "\n",
    "\n",
    "\n",
    "confidence_interval = 95\n",
    "labels = [\"Dense\", \"Contraction = 5\", \"Contraction = 20\", \"Contraction = 120\", \"Sparse\", \"Curriculum\"]\n",
    "\n",
    "\n",
    "\n",
    "perf_1_mean = np.mean(perf_1, axis=0)\n",
    "perf_1_std = np.std(perf_1, axis=0)\n",
    "perf_2_mean = np.mean(perf_2, axis=0)\n",
    "perf_2_std = np.std(perf_2, axis=0)\n",
    "perf_3_mean = np.mean(perf_3, axis=0)\n",
    "perf_3_std = np.std(perf_3, axis=0)\n",
    "perf_4_mean = np.mean(perf_4, axis=0)\n",
    "perf_4_std = np.std(perf_4, axis=0)\n",
    "perf_5_mean = np.mean(perf_5, axis=0)\n",
    "perf_5_std = np.std(perf_5, axis=0)\n",
    "perf_6_mean = np.mean(perf_6, axis=0)\n",
    "perf_6_std = np.std(perf_6, axis=0)\n",
    "\n",
    "perf_1_lower = np.percentile(perf_1, (100 - confidence_interval) / 2, axis=0)\n",
    "perf_1_upper = np.percentile(perf_1, 100 - (100 - confidence_interval) / 2, axis=0)\n",
    "perf_2_lower = np.percentile(perf_2, (100 - confidence_interval) / 2, axis=0)\n",
    "perf_2_upper = np.percentile(perf_2, 100 - (100 - confidence_interval) / 2, axis=0)\n",
    "perf_3_lower = np.percentile(perf_3, (100 - confidence_interval) / 2, axis=0)\n",
    "perf_3_upper = np.percentile(perf_3, 100 - (100 - confidence_interval) / 2, axis=0)\n",
    "perf_4_lower = np.percentile(perf_4, (100 - confidence_interval) / 2, axis=0)\n",
    "perf_4_upper = np.percentile(perf_4, 100 - (100 - confidence_interval) / 2, axis=0)\n",
    "perf_5_lower = np.percentile(perf_5, (100 - confidence_interval) / 2, axis=0)\n",
    "perf_5_upper = np.percentile(perf_5, 100 - (100 - confidence_interval) / 2, axis=0)\n",
    "perf_6_lower = np.percentile(perf_6, (100 - confidence_interval) / 2, axis=0)\n",
    "perf_6_upper = np.percentile(perf_6, 100 - (100 - confidence_interval) / 2, axis=0)\n",
    "\n",
    "plt.figure()\n",
    "# Plot the mean with error bars\n",
    "plt.plot(eps, perf_1_mean, label=labels[0])\n",
    "plt.fill_between(eps, perf_1_lower, perf_1_upper, alpha=0.3)\n",
    "plt.plot(eps, perf_2_mean, label=labels[1])\n",
    "plt.fill_between(eps, perf_2_lower, perf_2_upper, alpha=0.3)\n",
    "plt.plot(eps, perf_3_mean, label=labels[2])\n",
    "plt.fill_between(eps, perf_3_lower, perf_3_upper, alpha=0.3)\n",
    "plt.plot(eps, perf_4_mean, label=labels[3])\n",
    "plt.fill_between(eps, perf_4_lower, perf_4_upper, alpha=0.3)\n",
    "plt.plot(eps, perf_5_mean, label=labels[4])\n",
    "plt.fill_between(eps, perf_5_lower, perf_5_upper, alpha=0.3)\n",
    "plt.plot(eps, perf_6_mean, label=labels[5])\n",
    "plt.fill_between(eps, perf_6_lower, perf_6_upper, alpha=0.3)\n",
    "\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"performance (probability of reaching goal)\")\n",
    "plt.title(\"performance over time\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{eval_folder}/eval_over_time.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create side by side subplots of the 5 reward functions\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"Reward landscapes\", fontsize=16)\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(env_1.reward_landscape.T, origin='lower', cmap='gray')\n",
    "# hide the axes\n",
    "plt.axis('off')\n",
    "plt.title(labels[0])\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(env_2.reward_landscape.T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[1])\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(env_3.reward_landscape.T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[2])\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(env_4.reward_landscape.T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[3])\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(env_5.reward_landscape.T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[4])\n",
    "plt.savefig(f\"{eval_folder}/rewards_compare.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# create side by side subplots of the 6 Q functions\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"Q landscapes\", fontsize=16)\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(np.max(Q_1[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "# hide the axes\n",
    "plt.axis('off')\n",
    "plt.title(labels[0])\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(np.max(Q_2[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[1])\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(np.max(Q_3[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[2])\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(np.max(Q_4[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[3])\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(np.max(Q_5[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[4])\n",
    "plt.subplot(2,3,6)\n",
    "plt.imshow(np.max(Q_6[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[5])\n",
    "plt.savefig(f\"{eval_folder}/Q_compare.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# create side by side subplots of the 5 Q functions\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"Best Actions\", fontsize=16)\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(np.argmax(Q_1[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "# hide the axes\n",
    "plt.axis('off')\n",
    "plt.title(labels[0])\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(np.argmax(Q_2[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[1])\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(np.argmax(Q_3[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[2])\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(np.argmax(Q_4[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[3])\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(np.argmax(Q_5[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[4])\n",
    "plt.subplot(2,3,6)\n",
    "plt.imshow(np.argmax(Q_6[0].numpy(), axis=1).reshape(env_1.Nx,env_1.Ny).T,origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(labels[5])\n",
    "plt.savefig(f\"{eval_folder}/best_action_compare.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = env_1.test_Q_once(Q=Q_1[0].numpy(), episode_length=400, final_greediness=0.8, eps_anneal=False, use_start_zone=False, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = env_2.test_Q_once(Q=Q_2[0].numpy(), episode_length=400, final_greediness=0.8, eps_anneal=False, use_start_zone=False, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = env_3.test_Q_once(Q=Q_3[0].numpy(), episode_length=400, final_greediness=0.8, eps_anneal=False, use_start_zone=False, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = env_4.test_Q_once(Q=Q_4[0].numpy(), episode_length=400, final_greediness=0.8, eps_anneal=False, use_start_zone=False, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = env_5.test_Q_once(Q=Q_5[0].numpy(), episode_length=400, final_greediness=0.8, eps_anneal=False, use_start_zone=False, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = env_5.test_Q_once(Q=Q_5[0].numpy(), episode_length=400, final_greediness=0.8, eps_anneal=False, use_start_zone=False, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
