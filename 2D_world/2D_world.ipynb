{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import cv2\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_env():\n",
    "    # note the array is flipped when plotted\n",
    "    def __init__(self,sparse=True,model=None, move_penalty=0.0, goal_reward=1.0, collision_penalty=0.0):\n",
    "        self.N = 20 # sidelength of the grid map\n",
    "        \n",
    "        x = np.arange(self.N) # create x coord array\n",
    "        y = np.arange(self.N) # create y coord array\n",
    "\n",
    "        self.xx,self.yy = np.meshgrid(x,y) # create matrices of x and y coords, as separate matrices, that can be served as input to \n",
    "        # some multidimensional function\n",
    "\n",
    "        # create an occupancy map\n",
    "        self.occ_map = np.zeros((self.N,self.N))\n",
    "        # create a small hollow box in the middle of the map, with value 1, size 7x7, and a small opening\n",
    "        self.occ_map[6:14,6:14] = 1\n",
    "        self.occ_map[7:13,7:13] = 0\n",
    "        # add the small 2-pixel opening on the right side of the box\n",
    "        self.occ_map[8:10,13] = 0\n",
    "\n",
    "        # goal location\n",
    "        self.xl = 11 \n",
    "        self.yl = 9\n",
    "        \n",
    "        # reward\n",
    "        self.fv = lambda x,y : np.exp(-1*((x-self.xl)**2+(y-self.yl)**2)) # e^(-dist^2). gaussian proximity reward function\n",
    "        if sparse:\n",
    "            self.f = self.fv(self.yy, self.xx) # reward vector\n",
    "        #else:\n",
    "            #real_valued_states = torch.arange(-2.0, 2.01, 0.01).unsqueeze(1)\n",
    "            #rewards = model.predict_reward(real_valued_states)\n",
    "            #self.f = rewards[:, 0].detach().numpy()\n",
    "\n",
    "        # random initial location outside the box\n",
    "        # continually attempt to find a random initial location outside the box until one is found\n",
    "        self.state = self.search_for_free_state()\n",
    "        \n",
    "        self.action_space = np.array([[0,1],[0,-1],[-1,0],[1,0]]) # up down left right\n",
    "        self.move_penalty = move_penalty\n",
    "        self.collision_penalty = collision_penalty\n",
    "        self.goal_reward = goal_reward\n",
    "        \n",
    "    def search_for_free_state(self):\n",
    "        while True:\n",
    "            x0 = np.random.randint(self.N)\n",
    "            y0 = np.random.randint(self.N)\n",
    "            if self.occ_map[x0,y0] == 0:\n",
    "                break\n",
    "        return np.array([x0,y0])\n",
    "    # apply action and update the state\n",
    "    def mm(self,X,u):\n",
    "        collision = False\n",
    "        x = X[0] + u[0]\n",
    "        y = X[1] + u[1]\n",
    "        # if the new state is at an obstacle or out of the map, stay at the same state\n",
    "        if x < 0 or x > self.N-1 or y < 0 or y > self.N-1 or self.occ_map[x,y] == 1:\n",
    "            collision = True\n",
    "            return X, collision\n",
    "        else:\n",
    "            return np.array([x,y]), collision\n",
    "    \n",
    "    # step one interaction\n",
    "    def step(self,idx):\n",
    "        u = self.action_space[idx,:]\n",
    "        new_state, collision = self.mm(self.state,u)\n",
    "        self.state = np.copy(new_state)\n",
    "        # compute reward\n",
    "        reward = self.f[new_state[0],new_state[1]]\n",
    "        # subtract movement penalty\n",
    "        reward = reward - self.move_penalty\n",
    "        # subtract collision penalty\n",
    "        if collision:\n",
    "            reward = reward - self.collision_penalty\n",
    "        # return new state, gaussian proximity reward\n",
    "        return self.state, reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.search_for_free_state()\n",
    "        return np.copy(self.state)\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(self.occ_map.T, origin=\"lower\", cmap='gray')\n",
    "        plt.plot(self.state[0],self.state[1],'ro') # agent location\n",
    "        plt.plot(self.xl,self.yl,'gx') # goal location\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_reward(self):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(self.f.T, origin=\"lower\", cmap='gray')\n",
    "        plt.plot(self.state[0],self.state[1],'ro')\n",
    "\n",
    "    def plot_reward_and_trajectory(self, trajectory):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(self.f.T, origin=\"lower\", cmap='gray')\n",
    "        plt.plot(self.state[0],self.state[1],'ro')\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grid_env(sparse=True)\n",
    "env.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env,init=None,iters=10000,alpha=0.9,gamma=0.9,eps_greedy_init=0.5,eps_anneal=True,plot_freq=1,disp=True):\n",
    "    \n",
    "    if init is None:\n",
    "        Q = np.ones((env.N*env.N,env.action_space.shape[0])) # initialise Q table\n",
    "    else:\n",
    "        Q = init\n",
    "    reward = 0\n",
    "    if disp:\n",
    "        plt.figure(figsize=(15,5))\n",
    "    xm = []\n",
    "    rewards = []\n",
    "    for j in range(iters):\n",
    "        state = np.copy(env.state)\n",
    "        \n",
    "        # Epsilon-greedy\n",
    "        if eps_anneal:\n",
    "            eps_greedy = eps_greedy_init*np.exp(1-iters/(j+1))\n",
    "        else:\n",
    "            eps_greedy = eps_greedy_init\n",
    "            \n",
    "        if np.random.rand() < eps_greedy:\n",
    "            a = np.argmax(Q[env.N*state[0]+state[1],:])\n",
    "        else:\n",
    "            a = np.random.randint(env.action_space.shape[0])\n",
    "\n",
    "        new_state, new_reward = env.step(a)\n",
    "        xm.append(np.copy(new_state))\n",
    "\n",
    "        new_a = np.argmax(Q[env.N*new_state[0]+new_state[1],:])\n",
    "        Qmax = Q[env.N*new_state[0]+new_state[1],new_a]\n",
    "        Q[env.N*state[0]+state[1],a] = (1-alpha)*Q[env.N*state[0]+state[1],a] + alpha*(reward + gamma*Qmax)\n",
    "        reward = new_reward\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if (j %plot_freq == 0) and (disp):\n",
    "            plt.subplot(2,3,1)\n",
    "            plt.imshow(env.f.T,origin='lower', cmap='gray')\n",
    "#             plt.plot(np.vstack(xm)[:,0],np.vstack(xm)[:,1])\n",
    "            plt.title('True reward')\n",
    "\n",
    "            plt.subplot(2,3,2)\n",
    "            plt.imshow(np.max(Q,axis=1).reshape(env.N,env.N).T,origin='lower', cmap='gray')\n",
    "#             plt.plot(np.vstack(xm)[:,0],np.vstack(xm)[:,1])\n",
    "            plt.title('Q value')\n",
    "\n",
    "            plt.subplot(2,3,3)\n",
    "            plt.imshow(np.argmax(Q,axis=1).reshape(env.N,env.N).T,origin='lower', cmap='jet')\n",
    "#             plt.plot(np.vstack(xm)[:,0],np.vstack(xm)[:,1])\n",
    "            plt.title('Best action')\n",
    "\n",
    "            plt.subplot(2,1,2)\n",
    "            plt.plot(rewards,'o',alpha=0.01)\n",
    "            plt.ylabel('Reward')\n",
    "            plt.xlabel('Env interaction')\n",
    "            display.clear_output(wait=True)\n",
    "            plt.show()\n",
    "            print (eps_greedy)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new env with estimated reward and train agent\n",
    "new_env = grid_env()\n",
    "newQ = value_iteration(new_env,iters=20000,alpha=0.5,gamma=0.99,eps_greedy_init=0.99,eps_anneal=True,disp=True,plot_freq=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the env, Q table, episode length\n",
    "# plays one episode\n",
    "# plots the reward and q value for every step. \n",
    "def test_value(env,Q,steps=50,disp=True):\n",
    "    state = env.reset()\n",
    "    xm = [] # state\n",
    "    rsum = 0 # total reward\n",
    "    for j in range(steps):\n",
    "        a = np.argmax(Q[env.N*state[1]+state[0],:]) # select the best action\n",
    "\n",
    "        state,reward = env.step(a)\n",
    "    \n",
    "        xm.append(np.copy(state))\n",
    "        rsum = rsum+reward\n",
    "        if disp:\n",
    "            plt.clf()\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(env.f.T, origin=\"lower\",extent=[0,env.N,0,env.N])\n",
    "            plt.title('Actual reward')\n",
    "            plt.plot(np.vstack(xm)[:,0],np.vstack(xm)[:,1])\n",
    "\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(np.max(Q,axis=1).reshape(env.N,env.N).T, origin=\"lower\",extent=[0,env.N,0,env.N])\n",
    "            plt.title('Q estimate')\n",
    "            plt.plot(np.vstack(xm)[:,0],np.vstack(xm)[:,1],'-o')\n",
    "            plt.title(j)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "    return rsum, xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent on original world\n",
    "rsum,traj = test_value(new_env,newQ,steps=50,disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
